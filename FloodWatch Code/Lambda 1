import json
import boto3
import csv
import logging
from io import StringIO
from botocore.exceptions import ClientError

# Set up logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize S3 client
s3 = boto3.client('s3')

def create_s3_bucket(bucket_name):
    """Create an S3 bucket if it doesn't exist."""
    try:
        s3.head_bucket(Bucket=bucket_name)
        logger.info(f"Bucket {bucket_name} already exists.")
    except ClientError as e:
        if e.response['Error']['Code'] == '404':
            # The bucket does not exist
            try:
                s3.create_bucket(Bucket=bucket_name)
                logger.info(f"Bucket {bucket_name} created successfully.")
            except ClientError as ce:
                logger.error(f"Failed to create bucket {bucket_name}: {str(ce)}")
                raise ce
        else:
            logger.error(f"Error checking bucket {bucket_name}: {str(e)}")
            raise e

def lambda_handler(event, context):
    # Extract bucket name and key from event
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # Define the processed data bucket
    processed_bucket = "processed-data-bucket-name"
    
    # Ensure the processed data bucket exists
    create_s3_bucket(processed_bucket)
    
    # Log event details
    logger.info(f"New object uploaded: {key} in bucket: {bucket}")
    
    try:
        # Download the file from S3
        obj = s3.get_object(Bucket=bucket, Key=key)
        body = obj['Body'].read().decode('utf-8')
        
        # Read the CSV file
        csv_reader = csv.reader(StringIO(body))
        
        # Process the data
        processed_data = []
        for row in csv_reader:
            # Example processing: skip header and filter out rows with empty fields
            if csv_reader.line_num == 1:
                processed_data.append(row)  # keep the header
            elif all(row):
                processed_data.append(row)
        
        # Convert processed data back to CSV format
        output_csv = StringIO()
        csv_writer = csv.writer(output_csv)
        csv_writer.writerows(processed_data)
        output_csv.seek(0)

        # Define the processed file path
        processed_key = f"processed/{key}"
        
        # Upload the processed file to the processed bucket
        s3.put_object(Bucket=processed_bucket, Key=processed_key, Body=output_csv.getvalue())
        logger.info(f"Processed file uploaded to: {processed_key} in bucket: {processed_bucket}")
        
    except Exception as e:
        logger.error(f"Error processing file {key} from bucket {bucket}: {str(e)}")
        raise e
    
    return {
        'statusCode': 200,
        'body': json.dumps(f'Data processed successfully for file {key}')
    }